"""
HeartMuLa API Server
åŸºäºå®˜æ–¹ heartlib åº“çš„ FastAPI æœåŠ¡

å‚è€ƒæ–‡æ¡£ï¼š
- GitHub: https://github.com/HeartMuLa/heartlib
- HuggingFace: https://hf-mirror.com/HeartMuLa/HeartMuLa-oss-3B

éƒ¨ç½²æ–¹å¼ï¼š
- Docker é•œåƒåªåŒ…å«ä»£ç å’Œä¾èµ–
- æ¨¡å‹é€šè¿‡å…±ç»©ç®—åŠ›å­˜å‚¨å·æŒ‚è½½åˆ° /data/ckpt
- é¦–æ¬¡å¯åŠ¨è‡ªåŠ¨ä¸‹è½½æ¨¡å‹åˆ°å­˜å‚¨å·

API ç«¯ç‚¹ï¼š
- POST /generate/music  - æäº¤éŸ³ä¹ç”Ÿæˆä»»åŠ¡
- GET  /jobs/{job_id}   - æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
- GET  /download_track/{job_id} - ä¸‹è½½ç”Ÿæˆçš„éŸ³é¢‘
- DELETE /jobs/{job_id} - åˆ é™¤ä»»åŠ¡
"""

import os
import gc
import uuid
import asyncio
import subprocess
import sys
from datetime import datetime
from typing import Optional, Dict, Any
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# ============== é…ç½® ==============
# æ¨¡å‹è·¯å¾„ - æŒ‚è½½å­˜å‚¨å·çš„ç›®å½•
MODEL_PATH = os.environ.get("MODEL_PATH", "/data/ckpt")
OUTPUT_DIR = os.environ.get("OUTPUT_DIR", "/tmp/heartmula_outputs")
MAX_CONCURRENT_JOBS = int(os.environ.get("MAX_CONCURRENT_JOBS", "2"))
MODEL_VERSION = os.environ.get("MODEL_VERSION", "3B")

# GPU è®¾å¤‡é…ç½®
MULA_DEVICE = os.environ.get("MULA_DEVICE", "cuda:0")
CODEC_DEVICE = os.environ.get("CODEC_DEVICE", "cuda:0")

# æ˜¯å¦ä½¿ç”¨æ‡’åŠ è½½ï¼ˆå•GPUå†…å­˜ä¸è¶³æ—¶å¼€å¯ï¼‰
LAZY_LOAD = os.environ.get("LAZY_LOAD", "false").lower() == "true"

# HuggingFace é•œåƒï¼ˆå›½å†…åŠ é€Ÿï¼‰
HF_MIRROR = os.environ.get("HF_ENDPOINT", "https://hf-mirror.com")

# ============== ä»»åŠ¡å­˜å‚¨ ==============
jobs: Dict[str, Dict[str, Any]] = {}

# çº¿ç¨‹æ± ç”¨äºæ‰§è¡Œé˜»å¡çš„æ¨¡å‹æ¨ç†
executor = ThreadPoolExecutor(max_workers=MAX_CONCURRENT_JOBS)

# æ¨¡å‹åŠ è½½çŠ¶æ€
model_status = {
    "loaded": False,
    "loading": False,
    "error": None
}

# ============== Pydantic æ¨¡å‹ ==============
class GenerateMusicRequest(BaseModel):
    """éŸ³ä¹ç”Ÿæˆè¯·æ±‚å‚æ•° - ä¸¥æ ¼éµå¾ª heartlib å®˜æ–¹å‚æ•°"""
    prompt: str = Field(..., description="ç”Ÿæˆæç¤ºè¯/æè¿°")
    lyrics: str = Field(default="[instrumental]", description="æ­Œè¯å†…å®¹ï¼Œæ ¼å¼å‚è€ƒå®˜æ–¹æ–‡æ¡£")
    tags: Optional[str] = Field(default=None, description="é£æ ¼æ ‡ç­¾ï¼Œé€—å·åˆ†éš”å¦‚: piano,happy,romantic")
    duration_ms: int = Field(default=60000, ge=1000, le=240000, description="éŸ³é¢‘æ—¶é•¿(æ¯«ç§’)ï¼Œæœ€å¤§240000")
    
    # é«˜çº§å‚æ•° - æ¥è‡ªå®˜æ–¹ run_music_generation.py
    topk: int = Field(default=50, ge=1, le=500, description="Top-k é‡‡æ ·å‚æ•°")
    temperature: float = Field(default=1.0, ge=0.1, le=2.0, description="é‡‡æ ·æ¸©åº¦")
    cfg_scale: float = Field(default=1.5, ge=1.0, le=5.0, description="Classifier-free guidance scale")
    seed: Optional[int] = Field(default=None, description="éšæœºç§å­ï¼Œç”¨äºå¤ç°")


class JobStatus(BaseModel):
    """ä»»åŠ¡çŠ¶æ€å“åº”"""
    id: str
    status: str  # queued, processing, completed, failed
    error_msg: Optional[str] = None
    audio_path: Optional[str] = None
    generation_time_seconds: Optional[float] = None
    prompt: Optional[str] = None
    lyrics: Optional[str] = None
    tags: Optional[str] = None
    duration_ms: Optional[int] = None
    created_at: Optional[str] = None


# ============== FastAPI åº”ç”¨ ==============
app = FastAPI(
    title="HeartMuLa API",
    description="åŸºäº HeartMuLa å¼€æºæ¨¡å‹çš„éŸ³ä¹ç”Ÿæˆ API æœåŠ¡",
    version="1.0.0"
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============== æ¨¡å‹ä¸‹è½½ä¸åŠ è½½ ==============

def check_models_exist() -> bool:
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨äºå­˜å‚¨å·"""
    required_paths = [
        Path(MODEL_PATH) / "gen_config.json",
        Path(MODEL_PATH) / "tokenizer.json",
        Path(MODEL_PATH) / "HeartMuLa-oss-3B",
        Path(MODEL_PATH) / "HeartCodec-oss",
    ]
    
    for p in required_paths:
        if not p.exists():
            print(f"[HeartMuLa] ç¼ºå°‘æ–‡ä»¶: {p}")
            return False
    
    print(f"[HeartMuLa] æ¨¡å‹æ–‡ä»¶å·²å­˜åœ¨äºå­˜å‚¨å·: {MODEL_PATH}")
    return True


def download_models():
    """
    ä¸‹è½½æ¨¡å‹åˆ°å­˜å‚¨å·
    ä½¿ç”¨ HuggingFace å›½å†…é•œåƒåŠ é€Ÿ
    """
    print(f"[HeartMuLa] å¼€å§‹ä¸‹è½½æ¨¡å‹åˆ°å­˜å‚¨å·: {MODEL_PATH}")
    print(f"[HeartMuLa] ä½¿ç”¨é•œåƒ: {HF_MIRROR}")
    
    # è®¾ç½® HuggingFace é•œåƒ
    os.environ["HF_ENDPOINT"] = HF_MIRROR
    
    try:
        from huggingface_hub import snapshot_download
        
        model_path = Path(MODEL_PATH)
        model_path.mkdir(parents=True, exist_ok=True)
        
        # ä¸‹è½½åˆ—è¡¨
        downloads = [
            ("HeartMuLa/HeartMuLaGen", str(model_path)),
            ("HeartMuLa/HeartMuLa-RL-oss-3B-20260123", str(model_path / "HeartMuLa-oss-3B")),
            ("HeartMuLa/HeartCodec-oss-20260123", str(model_path / "HeartCodec-oss")),
        ]
        
        for repo_id, local_dir in downloads:
            print(f"[HeartMuLa] ä¸‹è½½ {repo_id} -> {local_dir}")
            snapshot_download(
                repo_id=repo_id,
                local_dir=local_dir,
                local_dir_use_symlinks=False,
                resume_download=True
            )
            print(f"[HeartMuLa] âœ“ {repo_id} ä¸‹è½½å®Œæˆ")
        
        print(f"[HeartMuLa] æ‰€æœ‰æ¨¡å‹ä¸‹è½½å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"[HeartMuLa] æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


# ============== å…¨å±€æ¨¡å‹å®ä¾‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰==============
_model_instance = None
_model_lock = asyncio.Lock()


def cleanup_gpu_memory(pipeline=None):
    """
    æ¸…ç† GPU æ˜¾å­˜ï¼Œé˜²æ­¢ OOM
    å‚è€ƒ HeartMuLa-Studio çš„å®ç°
    """
    import torch
    try:
        # 1. å¦‚æœæœ‰ pipelineï¼Œå…ˆæ¸…ç† KV cache
        if pipeline is not None:
            try:
                if hasattr(pipeline, 'mula') and hasattr(pipeline.mula, 'reset_caches'):
                    pipeline.mula.reset_caches()
                    print("[HeartMuLa] KV cache å·²é‡ç½®")
            except Exception as e:
                print(f"[HeartMuLa] é‡ç½® KV cache æ—¶å‡ºé”™: {e}")
        
        # 2. Python åƒåœ¾å›æ”¶
        gc.collect()
        
        # 3. CUDA æ˜¾å­˜æ¸…ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # æ‰“å°å½“å‰æ˜¾å­˜çŠ¶æ€
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            free = torch.cuda.mem_get_info()[0] / 1024**3
            print(f"[HeartMuLa] æ˜¾å­˜çŠ¶æ€: å·²åˆ†é… {allocated:.2f}GB, å·²ä¿ç•™ {reserved:.2f}GB, å¯ç”¨ {free:.2f}GB")
        
        print("[HeartMuLa] GPU æ˜¾å­˜æ¸…ç†å®Œæˆ")
    except Exception as e:
        print(f"[HeartMuLa] æ¸…ç†æ˜¾å­˜æ—¶å‡ºé”™: {e}")


def get_model():
    """è·å–æˆ–åˆå§‹åŒ–æ¨¡å‹å®ä¾‹"""
    global _model_instance, model_status
    
    if _model_instance is None:
        model_status["loading"] = True
        
        try:
            print(f"[HeartMuLa] æ­£åœ¨åŠ è½½æ¨¡å‹...")
            print(f"[HeartMuLa] MODEL_PATH: {MODEL_PATH}")
            print(f"[HeartMuLa] VERSION: {MODEL_VERSION}")
            print(f"[HeartMuLa] MULA_DEVICE: {MULA_DEVICE}")
            print(f"[HeartMuLa] CODEC_DEVICE: {CODEC_DEVICE}")
            print(f"[HeartMuLa] LAZY_LOAD: {LAZY_LOAD}")
            
            # å¯¼å…¥ heartlibï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¯åŠ¨æ—¶å°±åŠ è½½å¤§æ¨¡å‹ï¼‰
            import torch
            from heartlib import HeartMuLaGenPipeline
            from heartlib.heartcodec.modeling_heartcodec import HeartCodec
            
            # ============ å…³é”®ä¿®å¤ï¼šMonkey patch HeartCodec.detokenize ============
            # é—®é¢˜ï¼šHeartMuLa ç”Ÿæˆçš„ token æœ‰æ—¶ä¼šè¶…å‡º codec è¯æ±‡è¡¨èŒƒå›´ï¼ˆ8192ï¼‰
            # æ¯”å¦‚ EOS tokenï¼ˆ8193ï¼‰ï¼Œè¿™ä¼šå¯¼è‡´ "index out of bounds" é”™è¯¯
            # è§£å†³ï¼šåœ¨ detokenize å‰å°† frames é™åˆ¶åœ¨æœ‰æ•ˆèŒƒå›´ [0, 8191]
            # å‚è€ƒï¼šhttps://github.com/fspecii/HeartMuLa-Studio
            
            _original_detokenize = HeartCodec.detokenize
            
            @torch.inference_mode()
            def _patched_detokenize(self, frames, *args, **kwargs):
                # Clamp frame indices to valid codec vocabulary range [0, 8191]
                # The model may generate special tokens (EOS=8193, etc.) that are out of bounds
                CODEC_VOCAB_SIZE = 8192  # From codec_config.json codebook_size
                frames_clamped = frames.clamp(0, CODEC_VOCAB_SIZE - 1)
                print(f"[HeartMuLa] Frames clamped to [0, {CODEC_VOCAB_SIZE - 1}]")
                return _original_detokenize(self, frames_clamped, *args, **kwargs)
            
            HeartCodec.detokenize = _patched_detokenize
            print(f"[HeartMuLa] HeartCodec.detokenize å·²æ‰“è¡¥ä¸ï¼Œä¿®å¤ index out of bounds é”™è¯¯")
            # ============ è¡¥ä¸ç»“æŸ ============
            
            # ============ å…³é”®ä¿®å¤ 2ï¼šSequential Offload ============
            # é—®é¢˜ï¼šHeartMuLa (3B) å ç”¨ ~23GB æ˜¾å­˜ï¼Œcodec è§£ç æ—¶æ²¡æœ‰è¶³å¤Ÿç©ºé—´
            # è§£å†³ï¼šåœ¨ postprocess (codec è§£ç ) å‰ï¼Œå…ˆæŠŠ HeartMuLa ç§»åˆ° CPU
            # å‚è€ƒï¼šhttps://github.com/fspecii/HeartMuLa-Studio
            
            from heartlib.pipelines.music_generation import HeartMuLaGenPipeline as _Pipeline
            _original_postprocess = _Pipeline.postprocess
            
            def _patched_postprocess(self, model_outputs, **kwargs):
                """åœ¨ codec è§£ç å‰ï¼Œé‡Šæ”¾ HeartMuLa æ˜¾å­˜"""
                import gc as _gc  # åœ¨å‡½æ•°å†…éƒ¨å¯¼å…¥ï¼Œç¡®ä¿å¯è§
                
                print(f"[HeartMuLa] Sequential Offload: é‡Šæ”¾ HeartMuLa æ˜¾å­˜...")
                
                # 1. é‡ç½® KV cache
                if hasattr(self, 'mula') and hasattr(self.mula, 'reset_caches'):
                    self.mula.reset_caches()
                    print(f"[HeartMuLa] KV cache å·²é‡ç½®")
                
                # 2. å°† HeartMuLa ç§»åˆ° CPU
                if hasattr(self, '_mula') and self._mula is not None:
                    self._mula.to("cpu")
                    print(f"[HeartMuLa] HeartMuLa å·²ç§»è‡³ CPU")
                
                # 3. æ¸…ç† GPU æ˜¾å­˜
                _gc.collect()
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
                # æ‰“å°æ˜¾å­˜çŠ¶æ€
                allocated = torch.cuda.memory_allocated() / 1024**3
                free = torch.cuda.mem_get_info()[0] / 1024**3
                print(f"[HeartMuLa] æ˜¾å­˜é‡Šæ”¾å: å·²åˆ†é… {allocated:.2f}GB, å¯ç”¨ {free:.2f}GB")
                
                # 4. ç¡®ä¿ HeartCodec åœ¨ GPU ä¸Šè¿›è¡Œè§£ç 
                if hasattr(self, '_codec') and self._codec is not None:
                    codec_device = next(self._codec.parameters()).device
                    if codec_device.type == "cpu":
                        self._codec.to(torch.device(CODEC_DEVICE))
                        print(f"[HeartMuLa] HeartCodec å·²ç§»è‡³ GPU è¿›è¡Œè§£ç ")
                
                # 5. è°ƒç”¨åŸå§‹ postprocessï¼ˆcodec è§£ç ï¼‰
                result = _original_postprocess(self, model_outputs, **kwargs)
                
                # 5. è§£ç å®Œæˆåï¼Œä¸ç«‹å³ç§»å› GPUï¼ˆä¼š OOMï¼‰
                # HeartMuLa ç•™åœ¨ CPUï¼Œä¸‹æ¬¡ç”Ÿæˆæ—¶å†ç§»å›
                print(f"[HeartMuLa] Codec è§£ç å®Œæˆï¼ŒHeartMuLa ä¿æŒåœ¨ CPU")
                
                return result
            
            _Pipeline.postprocess = _patched_postprocess
            print(f"[HeartMuLa] Pipeline.postprocess å·²æ‰“è¡¥ä¸ï¼Œå®ç° Sequential Offload")
            # ============ è¡¥ä¸ 2 ç»“æŸ ============
            
            # è®¾ç½®è®¾å¤‡
            device = {
                "mula": torch.device(MULA_DEVICE),
                "codec": torch.device(CODEC_DEVICE)
            }
            
            # è®¾ç½®æ•°æ®ç±»å‹
            dtype = {
                "mula": torch.bfloat16,
                "codec": torch.float32
            }
            
            _model_instance = HeartMuLaGenPipeline.from_pretrained(
                pretrained_path=MODEL_PATH,
                device=device,
                dtype=dtype,
                version=MODEL_VERSION,
                lazy_load=LAZY_LOAD
            )
            
            model_status["loaded"] = True
            model_status["loading"] = False
            print(f"[HeartMuLa] æ¨¡å‹åŠ è½½å®Œæˆ!")
            
        except Exception as e:
            model_status["loading"] = False
            model_status["error"] = str(e)
            import traceback
            traceback.print_exc()
            raise e
    
    return _model_instance


def generate_music_sync(job_id: str, request: GenerateMusicRequest):
    """
    åŒæ­¥æ‰§è¡ŒéŸ³ä¹ç”Ÿæˆï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰
    ä¸¥æ ¼éµå¾ª heartlib å®˜æ–¹ APIï¼ˆHeartMuLaGenPipelineï¼‰
    åŒ…å« OOM é˜²æŠ¤ï¼šç”Ÿæˆå‰åæ¸…ç†æ˜¾å­˜
    """
    pipeline = None
    lyrics_file = None
    tags_file = None
    
    try:
        jobs[job_id]["status"] = "processing"
        start_time = datetime.now()
        
        print(f"[HeartMuLa] Job {job_id} å¼€å§‹ç”Ÿæˆ...")
        print(f"[HeartMuLa] Prompt: {request.prompt}")
        print(f"[HeartMuLa] Lyrics: {request.lyrics[:100]}...")
        print(f"[HeartMuLa] Tags: {request.tags}")
        print(f"[HeartMuLa] Duration: {request.duration_ms}ms")
        
        # ============ ç”Ÿæˆå‰æ¸…ç†æ˜¾å­˜ ============
        # é˜²æ­¢æ˜¾å­˜ç¢ç‰‡åŒ–å¯¼è‡´ OOM
        cleanup_gpu_memory()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        output_path = os.path.join(OUTPUT_DIR, f"{job_id}.mp3")
        
        # è·å–æ¨¡å‹ pipeline
        pipeline = get_model()
        
        # ============ ç¡®ä¿ HeartMuLa åœ¨ GPU ä¸Š ============
        # ä¸Šæ¬¡ç”Ÿæˆå HeartMuLa å¯èƒ½ç•™åœ¨ CPUï¼Œéœ€è¦ç§»å› GPU
        import torch
        if hasattr(pipeline, '_mula') and pipeline._mula is not None:
            mula_device = next(pipeline._mula.parameters()).device
            target_device = torch.device(MULA_DEVICE)
            if mula_device != target_device:
                print(f"[HeartMuLa] HeartMuLa åœ¨ {mula_device}ï¼Œéœ€è¦ç§»åˆ° {target_device}")
                # å…ˆæŠŠ HeartCodec ç§»åˆ° CPU è…¾å‡ºç©ºé—´
                if hasattr(pipeline, '_codec') and pipeline._codec is not None:
                    pipeline._codec.to("cpu")
                    gc.collect()
                    torch.cuda.empty_cache()
                    print(f"[HeartMuLa] HeartCodec å·²ç§»è‡³ CPU")
                # å†æŠŠ HeartMuLa ç§»å› GPU
                pipeline._mula.to(target_device)
                print(f"[HeartMuLa] HeartMuLa å·²ç§»è‡³ {target_device}")
        
        # å‡†å¤‡è¾“å…¥ - HeartMuLaGenPipeline æ¥å—å­—ç¬¦ä¸²æˆ–æ–‡ä»¶è·¯å¾„
        # å¦‚æœæ˜¯çº¯æ–‡æœ¬ï¼Œç›´æ¥ä¼ é€’ï¼›å¦‚æœéœ€è¦æ–‡ä»¶ï¼Œåˆ›å»ºä¸´æ—¶æ–‡ä»¶
        lyrics_content = request.lyrics.lower()  # å¿…é¡»å°å†™
        
        # tags å¿…é¡»æ˜¯å°å†™ã€é€—å·åˆ†éš”ã€æ— ç©ºæ ¼çš„æ ¼å¼
        # ä¾‹å¦‚: "piano,happy,romantic" è€Œä¸æ˜¯ "piano, happy, romantic"
        tags_content = request.tags or "instrumental"
        tags_content = tags_content.lower().replace(" ", "")  # å°å†™å¹¶ç§»é™¤ç©ºæ ¼
        
        # HeartMuLaGenPipeline å¯ä»¥æ¥å—å­—ç¬¦ä¸²æˆ–æ–‡ä»¶è·¯å¾„
        # ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        lyrics_file = os.path.join(OUTPUT_DIR, f"{job_id}_lyrics.txt")
        with open(lyrics_file, "w", encoding="utf-8") as f:
            f.write(lyrics_content)
        
        tags_file = os.path.join(OUTPUT_DIR, f"{job_id}_tags.txt")
        with open(tags_file, "w", encoding="utf-8") as f:
            f.write(tags_content)
        
        # è°ƒç”¨ HeartMuLaGenPipeline
        # pipeline(inputs, **kwargs)
        inputs = {
            "lyrics": lyrics_file,
            "tags": tags_file,
        }
        
        pipeline(
            inputs,
            save_path=output_path,
            max_audio_length_ms=request.duration_ms,
            topk=request.topk,
            temperature=request.temperature,
            cfg_scale=request.cfg_scale,
        )
        
        # è®¡ç®—ç”Ÿæˆæ—¶é—´
        end_time = datetime.now()
        generation_time = (end_time - start_time).total_seconds()
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        jobs[job_id]["status"] = "completed"
        jobs[job_id]["audio_path"] = output_path
        jobs[job_id]["generation_time_seconds"] = generation_time
        
        print(f"[HeartMuLa] Job {job_id} å®Œæˆ! è€—æ—¶: {generation_time:.2f}s")
        
    except Exception as e:
        import traceback
        error_msg = str(e)
        print(f"[HeartMuLa] Job {job_id} å¤±è´¥: {error_msg}")
        traceback.print_exc()
        
        jobs[job_id]["status"] = "failed"
        jobs[job_id]["error_msg"] = error_msg
    
    finally:
        # ============ ç”Ÿæˆåæ¸…ç†ï¼ˆæ— è®ºæˆåŠŸæˆ–å¤±è´¥ï¼‰============
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if lyrics_file and os.path.exists(lyrics_file):
            try:
                os.remove(lyrics_file)
            except Exception:
                pass
        if tags_file and os.path.exists(tags_file):
            try:
                os.remove(tags_file)
            except Exception:
                pass
        
        # æ¸…ç† GPU æ˜¾å­˜ï¼ˆåŒ…æ‹¬ KV cacheï¼‰
        # è¿™æ˜¯é˜²æ­¢è¿ç»­ç”Ÿæˆæ—¶ OOM çš„å…³é”®
        cleanup_gpu_memory(pipeline)


# ============== API ç«¯ç‚¹ ==============

@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥"""
    return {
        "service": "HeartMuLa API",
        "status": "running",
        "model_path": MODEL_PATH,
        "version": MODEL_VERSION,
        "model_status": model_status
    }


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy", "model_status": model_status}


@app.get("/model/status")
async def get_model_status():
    """è·å–æ¨¡å‹çŠ¶æ€"""
    models_exist = check_models_exist()
    return {
        "models_downloaded": models_exist,
        "model_loaded": model_status["loaded"],
        "model_loading": model_status["loading"],
        "error": model_status["error"],
        "model_path": MODEL_PATH
    }


@app.post("/model/download")
async def trigger_model_download(background_tasks: BackgroundTasks):
    """
    æ‰‹åŠ¨è§¦å‘æ¨¡å‹ä¸‹è½½
    ç”¨äºé¦–æ¬¡éƒ¨ç½²æ—¶ä¸‹è½½æ¨¡å‹åˆ°å­˜å‚¨å·
    """
    if check_models_exist():
        return {"status": "already_exists", "message": "æ¨¡å‹å·²å­˜åœ¨äºå­˜å‚¨å·"}
    
    # åœ¨åå°ä¸‹è½½
    background_tasks.add_task(download_models)
    return {"status": "downloading", "message": "æ¨¡å‹ä¸‹è½½å·²å¼€å§‹ï¼Œè¯·ç¨åæŸ¥è¯¢ /model/status"}


@app.post("/generate/music")
async def generate_music(request: GenerateMusicRequest, background_tasks: BackgroundTasks):
    """
    æäº¤éŸ³ä¹ç”Ÿæˆä»»åŠ¡
    
    è¿”å› job_idï¼Œé€šè¿‡ /jobs/{job_id} æŸ¥è¯¢çŠ¶æ€
    """
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å°±ç»ª
    if not check_models_exist():
        raise HTTPException(
            status_code=503, 
            detail="æ¨¡å‹æœªä¸‹è½½ï¼Œè¯·å…ˆè°ƒç”¨ POST /model/download æˆ–ç­‰å¾…è‡ªåŠ¨ä¸‹è½½å®Œæˆ"
        )
    
    job_id = str(uuid.uuid4())
    
    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
    jobs[job_id] = {
        "id": job_id,
        "status": "queued",
        "error_msg": None,
        "audio_path": None,
        "generation_time_seconds": None,
        "prompt": request.prompt,
        "lyrics": request.lyrics,
        "tags": request.tags,
        "duration_ms": request.duration_ms,
        "created_at": datetime.now().isoformat()
    }
    
    # åœ¨åå°çº¿ç¨‹æ± ä¸­æ‰§è¡Œç”Ÿæˆä»»åŠ¡
    loop = asyncio.get_event_loop()
    loop.run_in_executor(executor, generate_music_sync, job_id, request)
    
    return {"job_id": job_id, "status": "queued"}


@app.get("/jobs/{job_id}")
async def get_job_status(job_id: str):
    """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"""
    if job_id not in jobs:
        raise HTTPException(status_code=404, detail="Job not found")
    
    return jobs[job_id]


@app.get("/download_track/{job_id}")
async def download_track(job_id: str):
    """ä¸‹è½½ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶"""
    if job_id not in jobs:
        raise HTTPException(status_code=404, detail="Job not found")
    
    job = jobs[job_id]
    
    if job["status"] != "completed":
        raise HTTPException(
            status_code=400, 
            detail=f"Audio not ready. Status: {job['status']}"
        )
    
    audio_path = job.get("audio_path")
    if not audio_path or not os.path.exists(audio_path):
        raise HTTPException(status_code=404, detail="Audio file not found")
    
    return FileResponse(
        audio_path,
        media_type="audio/mpeg",
        filename=f"{job_id}.mp3"
    )


@app.delete("/jobs/{job_id}")
async def delete_job(job_id: str):
    """åˆ é™¤ä»»åŠ¡åŠå…¶ç”Ÿæˆçš„æ–‡ä»¶"""
    if job_id not in jobs:
        raise HTTPException(status_code=404, detail="Job not found")
    
    job = jobs[job_id]
    
    # åˆ é™¤éŸ³é¢‘æ–‡ä»¶
    audio_path = job.get("audio_path")
    if audio_path and os.path.exists(audio_path):
        try:
            os.remove(audio_path)
        except Exception as e:
            print(f"[HeartMuLa] åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
    
    # ä»ä»»åŠ¡åˆ—è¡¨ä¸­ç§»é™¤
    del jobs[job_id]
    
    return {"status": "deleted", "job_id": job_id}


@app.get("/jobs")
async def list_jobs(limit: int = 50):
    """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡ï¼ˆè°ƒè¯•ç”¨ï¼‰"""
    job_list = list(jobs.values())[-limit:]
    return {"jobs": job_list, "total": len(jobs)}


# ============== å¯åŠ¨äº‹ä»¶ ==============

@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨æ—¶çš„åˆå§‹åŒ– - è‡ªåŠ¨ä¸‹è½½æ¨¡å‹"""
    import torch
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"[HeartMuLa] API æœåŠ¡å¯åŠ¨")
    print(f"[HeartMuLa] è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"[HeartMuLa] æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print(f"[HeartMuLa] æ¨¡å‹ç‰ˆæœ¬: {MODEL_VERSION}")
    print(f"[HeartMuLa] HF å›½å†…é•œåƒ: {HF_MIRROR}")
    print(f"[HeartMuLa] æœ€å¤§å¹¶å‘: {MAX_CONCURRENT_JOBS}")
    
    # ============ GPU æ˜¾å­˜ä¼˜åŒ–é…ç½® ============
    # è®¾ç½® PyTorch å†…å­˜åˆ†é…å™¨é…ç½®ï¼Œå‡å°‘æ˜¾å­˜ç¢ç‰‡åŒ–
    # å‚è€ƒ: https://pytorch.org/docs/stable/notes/cuda.html#environment-variables
    if "PYTORCH_CUDA_ALLOC_CONF" not in os.environ:
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
        print(f"[HeartMuLa] å·²è®¾ç½® PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")
    
    # æ˜¾ç¤º GPU ä¿¡æ¯
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"[HeartMuLa] GPU: {gpu_name} ({gpu_mem:.1f}GB)")
        
        # æ˜¾å­˜ä½¿ç”¨å»ºè®®
        if gpu_mem < 16:
            print(f"[HeartMuLa] âš ï¸ æ˜¾å­˜ < 16GBï¼Œå»ºè®®ä½¿ç”¨çŸ­æ—¶é•¿ï¼ˆduration_ms <= 30000ï¼‰")
        elif gpu_mem < 24:
            print(f"[HeartMuLa] ğŸ’¡ æ˜¾å­˜ < 24GBï¼Œå»ºè®®ä½¿ç”¨ä¸­ç­‰æ—¶é•¿ï¼ˆduration_ms <= 60000ï¼‰")
        else:
            print(f"[HeartMuLa] âœ“ æ˜¾å­˜å……è¶³ï¼Œå¯ç”Ÿæˆè¾ƒé•¿éŸ³é¢‘")
    else:
        print(f"[HeartMuLa] âš ï¸ æœªæ£€æµ‹åˆ° CUDA GPUï¼Œå°†ä½¿ç”¨ CPUï¼ˆéå¸¸æ…¢ï¼‰")
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™è‡ªåŠ¨ä¸‹è½½
    if check_models_exist():
        print(f"[HeartMuLa] æ¨¡å‹å·²å­˜åœ¨äºå­˜å‚¨å·ï¼ŒæœåŠ¡å°±ç»ª")
    else:
        print(f"[HeartMuLa] æ¨¡å‹ä¸å­˜åœ¨ï¼Œå¼€å§‹è‡ªåŠ¨ä¸‹è½½...")
        print(f"[HeartMuLa] ä½¿ç”¨å›½å†…é•œåƒ: {HF_MIRROR}")
        # åœ¨åå°çº¿ç¨‹ä¸‹è½½ï¼Œä¸é˜»å¡æœåŠ¡å¯åŠ¨
        loop = asyncio.get_event_loop()
        loop.run_in_executor(executor, download_models)


if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", "8000"))
    uvicorn.run(app, host="0.0.0.0", port=port)
